import torch
import glob
import os
from torch_geometric.io import read_txt_array
from torch_geometric.data.data import Data
import torch_geometric.transforms as T
from torch_geometric.nn import knn_interpolate
import numpy as np

from src.core.data_transform import SaveOriginalPosId
from src.datasets.base_dataset import BaseDataset
from src.metrics.shapenet_part_tracker import ShapenetPartTracker
from .shapenet import ShapeNet


class _ForwardShapenet(torch.utils.data.Dataset):
    """ Dataset to run forward inference on Shapenet kind of data data. Runs on a whole folder.
    Arguments:
        path: folder that contains a set of files of a given category
        category: category index of the files contained in this folder
        transforms: transforms to be applied to the data
        include_normals: wether to include normals for the forward inference
    """

    def __init__(self, path, category, transforms=None, include_normals=True):
        super().__init__()
        assert category < len(ShapeNet.category_ids)
        self._path = path
        self._files = glob.glob(os.path.join(self._path, "*.txt"))
        self._transforms = transforms
        self._include_normals = include_normals
        self._category = category
        assert os.path.exists(self._path)
        if self.__len__() == 0:
            raise ValueError("Empty folder %s" % path)

    def __len__(self):
        return len(self._files)

    def _read_file(self, filename):
        raw = read_txt_array(filename)
        pos = raw[:, :3]
        x = raw[:, 3:6]
        return Data(pos=pos, x=x)

    def get_raw(self, index):
        """ returns the untransformed data associated with an element
        """
        return self._read_file(self._files[index])

    @property
    def num_features(self):
        feats = self[0].x
        if feats is not None:
            return feats.shape[-1]
        return 0

    def get_filename(self, index):
        return os.path.basename(self._files[index])

    def __getitem__(self, index):
        data = self._read_file(self._files[index])
        data.y = None
        setattr(data, "sampleid", torch.tensor([index]))
        if not self._include_normals:
            data.x = None
        if self._transforms is not None:
            data = self._transforms(data)
        return data


class ForwardShapenetDataset(BaseDataset):
    def __init__(self, dataset_opt):
        super().__init__(dataset_opt)
        self._data_path = dataset_opt.dataroot
        include_normals = dataset_opt.include_normals if dataset_opt.include_normals else True

        transforms = SaveOriginalPosId()
        for t in [self.pre_transform, self.test_transform]:
            if t:
                transforms = T.Compose([transforms, t])
        self.test_dataset = _ForwardShapenet(
            self._data_path, dataset_opt.category, transforms=transforms, include_normals=include_normals
        )

    @staticmethod
    def get_tracker(model, dataset, wandb_log: bool, tensorboard_log: bool):
        """Factory method for the tracker

        Arguments:
            dataset {[type]}
            wandb_log - Log using weight and biases
        Returns:
            [BaseTracker] -- tracker
        """
        return ShapenetPartTracker(dataset, wandb_log=wandb_log, use_tensorboard=tensorboard_log)

    def predict_original_samples(self, batch, conv_type, output):
        """ Takes the output generated by the NN and upsamples it to the original data
        Arguments:
            batch -- processed batch
            conv_type -- Type of convolutio (DENSE, PARTIAL_DENSE, etc...)
            output -- output predicted by the model
        """
        full_res_results = {}
        num_sample = BaseDataset.get_num_samples(batch, conv_type)
        if conv_type == "DENSE":
            output = output.reshape(num_sample, -1, output.shape[-1])  # [B,N,L]

        setattr(batch, "_pred", output)
        for b in range(num_sample):
            sampleid = batch.sampleid[b]
            sample_raw_pos = self.test_dataset.get_raw(sampleid).pos.to(output.device)
            predicted = BaseDataset.get_sample(batch, "_pred", b, conv_type)
            origindid = BaseDataset.get_sample(batch, SaveOriginalPosId.KEY, b, conv_type)
            full_prediction = knn_interpolate(predicted, sample_raw_pos[origindid], sample_raw_pos, k=3)
            labels = full_prediction.max(1)[1].unsqueeze(-1)
            full_res_results[self.test_dataset.get_filename(sampleid)] = np.hstack(
                (sample_raw_pos.cpu().numpy(), labels.cpu().numpy(),)
            )
        return full_res_results
